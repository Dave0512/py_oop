{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verarbeitung GHX Kataloge (WorkFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Zugriff auf FTP\n",
    "#- In Klärung BW\n",
    "\n",
    "## 2. Liste alle zips in FTP\n",
    "\n",
    "## 3. Liste alls zips in GHX-Kataloge-Arbeitsverzeichnis\n",
    "\n",
    "## 4. Bestimme relevante (neue, noch nicht in GHX-Kataloge-Arbeitsverzeichnis aufgeführte) zips in FTP\n",
    "\n",
    "## 5. Kopiere relevante zips in GHX-Kataloge-Arbeitsverzeichnis\n",
    "\n",
    "## 6. Entpacke relevante zips \"chronologisch\" nach Datum im Dateinamen \n",
    "#- (Jede zip durchläuft nacheinander die Schritte 7 -9)\n",
    "\n",
    "## 7. Lese .csv Dateien in Datenbank ein\n",
    "\n",
    "## 8. Schreibe verarbeitete .zip in tbl_zips in die DB\n",
    "\n",
    "## 9. Lösche verarbeitete Ordner mit csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import zipfile\n",
    "import gc\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "import urllib\n",
    "import import_ipynb\n",
    "import ipynb\n",
    "import pyodbc\n",
    "import schedule\n",
    "from smb.SMBConnection import SMBConnection\n",
    "\n",
    "\n",
    "# Import eigene Klasse\n",
    "#import py_etl_sql_funktionen\n",
    "#from ipynb.fs.full.py_etl_sql_funktionen import sql_ausfuehrung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Liste alle zips in FTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lese Zip Dateien ab initialer zip aufsteigend\n",
    "- Versehe mit Zeitstempel und lade zukünftige csv Dateien nur ein, wenn noch nicht in Zip-Liste enthalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zugriff auf FTP\n",
    "- In Klärung BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = \"\".join([os.getcwd(), \"\\\\\"]) \n",
    "etl_path = \"Z:\\\\1_AGKAMED_Arbeit\\\\0_GIT_REPOS\\\\1_ETL\\\\\"\n",
    "ghx_cataloge_path = \"\".join([etl_path,\"GHX_Kataloge\\\\\"])\n",
    "# ghx_cataloge_path = \"\".join([current_path,\"GHX_Kataloge\\\\\"])\n",
    "# ftp_path = \"\".join([current_path,\"GHX_FTP\\\\\"]) \n",
    "ini_zip = 'CatUpd_AGKAMED_20200717030056.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(ftp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Liste alle zips in FTP\n",
    "\n",
    "## 3. Liste alls zips in GHX-Kataloge-Arbeitsverzeichnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanten für SMB Zugriff\n",
    "# userID = 'ftp-serv-ip3'\n",
    "# password = 'bnT0RAOo1DN5NkokQbCs'\n",
    "# client_machine_name = 'AGKVMPC6' # local PC Name\n",
    "# server_name = 'agkftp01'\n",
    "# server_ip = '192.168.209.42'\n",
    "shared_folder_name = 'Ghx_kataloge$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _smb_connector():\n",
    "    \"\"\"\n",
    "    Stellt Verbindung zu ftp-Ordner (GHX-Kataloge) via SMB her.\n",
    "    INPUT:\n",
    "        --\n",
    "    OUTPUT:\n",
    "        conn: Verbindung\n",
    "    \"\"\"\n",
    "    userID = 'ftp-serv-ip3'\n",
    "    password = 'bnT0RAOo1DN5NkokQbCs'\n",
    "    client_machine_name = 'AGKVMPC6' # local PC Name\n",
    "    server_name = 'agkftp01'\n",
    "    server_ip = '192.168.209.42'\n",
    "    \n",
    "    conn = SMBConnection(userID, password, client_machine_name, server_name, use_ntlm_v2=True,\n",
    "                         is_direct_tcp=True)\n",
    "\n",
    "    conn.connect(server_ip, 445) \n",
    "\n",
    "    if conn:\n",
    "        # print(\"\\nSMB-Verbindung zu {} erfolgreich: \".format(conn))\n",
    "        return conn\n",
    "\n",
    "    else:\n",
    "        print(\"SMB-Verbindung fehlerhaft.\")\n",
    "\n",
    "    \n",
    "    conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen zur Datei-Listung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dateien_in_ftp_smb():\n",
    "    \"\"\"\n",
    "    Auflistung von .zip-Dateien in SMB Ordner \n",
    "    INPUT: \n",
    "        SMB-Verbindung zum remote-ftp-Kataloge-Ordner\n",
    "    \"\"\"\n",
    "    initial_zip = ini_zip\n",
    "    conn = _smb_connector()\n",
    "    lst_smb = conn.listPath(shared_folder_name,'/',pattern='*.zip')\n",
    "    lst_smb_zips = [share.filename for share in lst_smb if share.filename[15:25] >= initial_zip[15:25]]\n",
    "    return lst_smb_zips # Liste aller im Ordner befindlichen .zips ab Initial-Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dateien_in_ordner(pfad,suffix='.zip'):\n",
    "    \"\"\"\n",
    "    Auflistung von Dateien jeglichen Typs in beliebigem Ordner\n",
    "    \n",
    "    INPUT: \n",
    "        - Ordner \n",
    "        - Dateityp (Vorbelegt mit zip) \"Alle Typen möglich\"\n",
    "    OUTPUT:\n",
    "        - Liste der Dateien des gewählten Typs in dem angegebenen Ordner\n",
    "    \"\"\"\n",
    "    \n",
    "    fil_lst = [fil for fil in os.listdir(pfad) if fil.endswith(suffix)]\n",
    "    return fil_lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _start_zip_handler():\n",
    "    \"\"\"\n",
    "    Prüfung, ob die gefundenen .zips Dateien in beiden Ordnern übereinstimmen.\n",
    "    Bei Übereinstimmung wird Programm beendet. \n",
    "    Sonst startet zip-Handler.\n",
    "    INPUT: \n",
    "        quellPfad,zielPfad = Vorbelegt mit SMB Ordner Inhalt und Arbeitsordner\n",
    "    OUTPUT:\n",
    "        boolscher Wert \n",
    "    \"\"\"\n",
    "    vgl_lsts = set(_dateien_in_ftp_smb()) == set(_dateien_in_ordner(ghx_cataloge_path))\n",
    "\n",
    "    return vgl_lsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_zip_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finde relevante (neue, noch nicht in GHX-Kataloge-Arbeitsverzeichnis aufgeführte) zips in FTP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _neustes_datum_ghx_zip(pfad):\n",
    "    \"\"\"\n",
    "    Lese aktuellstes Datum der bereits vorliegenden zips in GHX Kataloge Arbeitsordner.\n",
    "    Falls keine zip-Datei im GHX-Arbeitsordner wird \n",
    "    auf das Datum der Initialen zip zurückgegriffen.\n",
    "    \n",
    "    INPUT:\n",
    "        Liste mit ZIP-Dateien \"GHX-Kataloge\" \n",
    "    OUTPUT:\n",
    "        Aktuellstes Datum\n",
    "        \n",
    "    \"\"\"\n",
    "    initial_zip = ini_zip\n",
    "    if not _dateien_in_ordner(pfad):\n",
    "        return initial_zip[15:25]\n",
    "    else: \n",
    "        df_dat = pd.DataFrame(_dateien_in_ordner(pfad)\n",
    "                              ,dtype=str\n",
    "                              ,columns=['Zip_Name_ID'])\n",
    "        df_dat['Zip_Datum'] = df_dat['Zip_Name_ID'].str.slice(start=15, stop=25)\n",
    "        return df_dat['Zip_Datum'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _erstelle_inload_liste():\n",
    "    \"\"\"\n",
    "    Liste der relevanten ZIP Dateien erstellen, in Abhängigkeit\n",
    "    der bereits vorliegenden ZIPs im GHX-Kataloge-Arbeitsordner.\n",
    "    Liegt keine zip im Arbeitsordner werden alle im ftp befindlichen Dateien\n",
    "    ab dem intial-zip auf die Inload-Liste gesetzt und eingeladen.  \n",
    "    \n",
    "    INPUT:\n",
    "        quellPfad und zielPfad: ftp oder ghx-Arbeitsordner \n",
    "    OUTPUT:\n",
    "        Liste relevanter ZIPs für Inload (Typ: Liste)\n",
    "    \"\"\"\n",
    "    \n",
    "    relev_zips_lst = []\n",
    "\n",
    "    if not _dateien_in_ordner(ghx_cataloge_path): # Wenn GHX-ArbeitsOrdner leer ist\n",
    "        zip_dateien = _dateien_in_ftp_smb()\n",
    "        print(\"GHX-Arbeitsordner ist leer.\\nEs werden alle .zip-Dateien aus FTP\\n\"\n",
    "              \"ab dem Initialdatum geladen.\")\n",
    "        for datei in zip_dateien:\n",
    "            if datei[15:25] >= _neustes_datum_ghx_zip(ghx_cataloge_path):\n",
    "                try:\n",
    "                    relev_zips_lst.append({'Zip_Name_ID' : datei, \n",
    "                                   'Zip_Datum'   : datei[15:25],\n",
    "                                   'Zeitstempel' : dt.datetime.now()\n",
    "                                   })\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "        return relev_zips_lst\n",
    "    else: \n",
    "        zip_dateien = _dateien_in_ftp_smb()\n",
    "        for datei in zip_dateien:\n",
    "            if datei[15:25] > _neustes_datum_ghx_zip(ghx_cataloge_path):\n",
    "                try:\n",
    "                    relev_zips_lst.append({'Zip_Name_ID' : datei, \n",
    "                                   'Zip_Datum'   : datei[15:25],\n",
    "                                   'Zeitstempel' : dt.datetime.now()\n",
    "                                   })\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "        return relev_zips_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(_erstelle_inload_liste())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kopiere relevante zips in GHX-Kataloge-Arbeitsverzeichnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _kopiere_relev_zips(zipDF,zipNamenFeld):\n",
    "#     \"\"\"\n",
    "#     Kopiert alle relevanten .zip-Dateien auf ftp-Ordner in GHX-Kataloge-Arbeitsordner.\n",
    "    \n",
    "#     INPUT:\n",
    "#         quellPfad:   ftp, \n",
    "#         zielPfad:    ghx-Arbeitsordner (Vorbelegt),\n",
    "#         zipDF:       DF der relevanten zips (Erweiterter Scope), \n",
    "#         zipNamenFeld:Spaltenname der Zip-Dateien\n",
    "#     OUTPUT: \n",
    "#         --\n",
    "#     \"\"\"\n",
    "    \n",
    "#     df_relev_zips = pd.DataFrame(zipDF,dtype=str) # ok\n",
    "#     relev_zips = df_relev_zips[zipNamenFeld].tolist() # ok\n",
    "\n",
    "#     zips_ftp = _dateien_in_ftp_smb()\n",
    "    \n",
    "#     # #############\n",
    "#     # ANPASSEN! String \"quellPfad\" durch SMB-Verbindung\n",
    "#     # #############\n",
    "    \n",
    "#     [shutil.copy(quellPfad + fil,ghx_cataloge_path + fil) for fil in zips_ftp if fil in relev_zips] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kopiere_relev_zips_from_smb():\n",
    "        \"\"\" Download files from the remote share. \"\"\"\n",
    "        conn = _smb_connector()\n",
    "        df_smb_files = pd.DataFrame(_erstelle_inload_liste(),dtype=str) # Liste relevante Zips\n",
    "        server_name = 'agkftp01'\n",
    "        shared_folder_name = 'Ghx_kataloge$'\n",
    "        lst_smb_files = df_smb_files['Zip_Name_ID'].to_list()\n",
    "\n",
    "        for file in lst_smb_files:\n",
    "            try: \n",
    "                with open(file, 'wb') as file_obj:\n",
    "                    conn.retrieveFile(\n",
    "                                      service_name=shared_folder_name,\n",
    "                                      path=file,\n",
    "                                      file_obj=file_obj)\n",
    "                    \n",
    "                shutil.move(current_path + file, ghx_cataloge_path + file) # Schiebt .zip-Dateien von Programm-Verzeichnis ins Arbeitsverzeichnis.\n",
    "            except Exception as OpF:\n",
    "                print(\"\\n\" + str(OpF) + \"\\n\")\n",
    "        \n",
    "#         [shutil.move(current_path + fil,ghx_cataloge_path + fil) for fil in lst_smb_files] \n",
    "        print(\"\\n\\nZIPs wurden erfolgreich in den Arbeitsordner kopiert.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entpacke relevante zips \"chronologisch\" nach Datum im Dateinamen \n",
    "- (Jede zip durchläuft nacheinander die Schritte 7 -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_folder_from_zips(zipDF,zipNamenFeld):\n",
    "    \"\"\"\n",
    "    Entpacke ZIP's\n",
    "    \n",
    "    INPUT: \n",
    "        Pfad mit ZIP's\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    \n",
    "    df_relev_zips = pd.DataFrame(zipDF,dtype=str)\n",
    "    relev_zips = df_relev_zips[zipNamenFeld].tolist()\n",
    "    \n",
    "    for zip_file in relev_zips:\n",
    "        try:\n",
    "            with zipfile.ZipFile(ghx_cataloge_path + zip_file,'r') as myZip:\n",
    "                myZip.extractall(path=ghx_cataloge_path)\n",
    "                _fill_csv_in_ms_sql()\n",
    "                _del_fold_csv(ghx_cataloge_path)\n",
    "                gc.collect(generation=2)\n",
    "                print(\"Neu importierte zip: \" + zip_file)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Lese .csv Dateien in Datenbank ein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _verbinde_ms_sql():\n",
    "    server_login = urllib.parse.quote_plus(r\"DRIVER={SQL Server Native Client 11.0};\"\n",
    "                                r\"SERVER=192.168.16.124;\"\n",
    "                                r\"DATABASE=Vorlauf_DB;\"\n",
    "                                r\"Trusted_Connection=yes;\")   \n",
    "    server_verbindung = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(server_login))\n",
    "    return server_verbindung\n",
    "\n",
    "#                                r\"SERVER=192.168.16.124;\"\n",
    "#                                r\"DATABASE=Vorlauf_DB;\"\n",
    "#                                r\"SERVER=192.168.16.123;\"\n",
    "#                                r\"DATABASE=Vorlauf_DB;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(_verbinde_ms_sql()) # sqlalchemy.engine.base.Engine\n",
    "# type(_verbinde_zu_server_und_db()) # pyodbc.Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fill_csv_in_ms_sql():\n",
    "    \"\"\"\n",
    "    Fülle csv in etl Datenbank\n",
    "    INPUT:\n",
    "        csv:    GHX Kataloge        \n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    chunk_size = 10000\n",
    "    i = 1\n",
    "    j = 1\n",
    "    start_time = time.time() \n",
    "\n",
    "    server_verbindung = _verbinde_ms_sql()\n",
    "    alle_csv = glob.glob(\"**/*.csv\",recursive=True)\n",
    "    for csv_file in alle_csv:\n",
    "        try:\n",
    "            for df_csv in pd.read_csv(csv_file,\n",
    "                                sep='\\t',\n",
    "                                # dtype=str,\n",
    "                                dtype='unicode',\n",
    "                                lineterminator='\\r',\n",
    "                                iterator=True,\n",
    "                                chunksize=chunk_size,\n",
    "                                encoding='latin-1'\n",
    "                                ):\n",
    "                df_csv = df_csv.rename(columns = {c: c.replace(' ','') for c in df_csv.columns})\n",
    "                df_csv.index += j\n",
    "                # Spalten ergänzen / bereinigen\n",
    "                df_csv['TPSHORTNAME'] = df_csv['TPSHORTNAME'].str.replace(r'\\W','')\n",
    "                df_csv['_computed_normalized_suppliername_'] = df_csv['SUPPLIERNAME'].str.replace(r'\\W','')\n",
    "                df_csv['_katalog_inload_'] = dt.datetime.now()\n",
    "                df_csv['_computed_normalized_art_nr_'] = df_csv['SUPPLIERPARTNUM'].str.replace(r'\\W','')\n",
    "                df_csv['_artikel_key_inload_'] = df_csv['_computed_normalized_suppliername_'] + \"°\" + df_csv['_computed_normalized_art_nr_'] + \"°\" + df_csv['UOM'] + \"°\" + df_csv['NOU']\n",
    "                df_csv['_csv_file_'] = csv_file.split(\"\\\\\",1)[1]\n",
    "                df_csv['_prio_flag_'] = \"\"\n",
    "                df_csv['_hibc_'] = \"\"\n",
    "                df_csv['_gtin_'] = \"\"\n",
    "                df_csv['_ean_kommentar_'] = \"\"\n",
    "                # Übertrag aus sql_empty_cols\n",
    "                df_csv['_LieferantenNameGlobal_'] = \"\"\n",
    "                df_csv['_artikel_key_'] = \"\"\n",
    "                df_csv['_AGKA_Kreditoren_ID_'] = \"\"\n",
    "                df_csv['_artikel_key_class_'] = \"\"\n",
    "                df_csv['_agka_class_id_'] = \"\"\n",
    "                df_csv['_UStID_Vorschlag_'] = \"\"\n",
    "                \n",
    "                df_csv['_gtin_pruefziffer_'] = \"\"\n",
    "                df_csv['_gtin_pruefung_kommentar_'] = \"\"\n",
    "                \n",
    "                \n",
    "                df_csv['_AGKclassCode-LEVEL1_'] = \"\"\n",
    "                df_csv['_AGKclassCode-LEVEL2_'] = \"\"\n",
    "                df_csv['_AGKclassCode-LEVEL3_'] = \"\"\n",
    "                df_csv['_AGKclassCode-LEVEL4_'] = \"\"\n",
    "                \n",
    "#                df_csv['tbl_index'] = \"\"\n",
    "                \n",
    "                #df_inload_verw = df_csv[['TPSHORTNAME','_artikel_key_inload_','_csv_file_','_katalog_inload_']]\n",
    "                #df_csv = df_csv.dropna(subset = ['MFRNAME','SUPPLIERNAME','SUPPLIERPARTNUM','NOU','UOM'],inplace=True) #  Zeilen mit Nullwerten ignorieren\n",
    "                df_csv.to_sql(\"tbl_ghx_kataloge_roh\",con=server_verbindung,if_exists='append',index=False)\n",
    "                #df_inload_verw.to_sql(\"tbl_ghx_inload_verw\",con=server_verbindung,if_exists='append',index=False)\n",
    "                j = df_csv.index[-1]+1\n",
    "                end_time = time.time()\n",
    "                print(\"Dauer je Chunk (Zeilen-Paket): \", end_time - start_time)\n",
    "\n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(csv_file)\n",
    "            print(e)\n",
    "            continue            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Lese xlsx Dateien in Datenbank ein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xlsx Tabellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_tab_names = [\"tbl_covin_validierung\",\n",
    "                  \"tbl_lieferanten\",\n",
    "                  \"tbl_lieferanten_master\",\n",
    "                  \"tbl_artikel_agkaClass_jd\",\n",
    "                  \"tbl_agkaClass_jd\",\n",
    "                  \"tbl_warenkorb\",\n",
    "                  \"tbl_abdata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zu verändernde Spalten je xlsx Tabelle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictSpaltenZurAnpassung = { \"tbl_warenkorb\":(   'LieferantenNrAGKA',\n",
    "                                                'LieferantenNameGlobal',\n",
    "                                                'LieferantenArtikelNr'),\n",
    "                            \"tbl_lieferanten_master\":( 'TPSHORTNAME',\n",
    "                                                'LieferantenNr',\n",
    "                                                'MiiLieferantGlobal',\n",
    "                                                'Finale_UStID'),\n",
    "                            \"tbl_artikel_agkaClass_jd\":('Lieferantennummer_vom_Lieferanten',\n",
    "                                                        'Artikelnummer_vom_Lieferanten'),\n",
    "                            \"tbl_lieferanten_ueberl\":('LieferantenNr',\n",
    "                                                      'Nachfolger'),\n",
    "                            \"tbl_lieferanten_kataloge\":('TPSHORTNAME',\n",
    "                                                        'LieferantenNr')}\n",
    "# tbl_warenkorb add 'LieferantenNrAGKA',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _import_xlsx_conv_to_df(tabelle):\n",
    "    \"\"\"\n",
    "    Einlesen xlsx-Stammdatentabellen in Vorlauf_DB Datenbank\n",
    "    INPUT:\n",
    "        Tabellenname (.xlsx)\n",
    "    OUTPUT:\n",
    "        xlsx-tab als Datenframe\n",
    "    \"\"\"\n",
    "    df_xlsx = pd.read_excel(current_path + tabelle + \".xlsx\",sheet_name=\"_Importdaten\",dtype=str)\n",
    "    df_xlsx = df_xlsx.assign(tbl_index=df_xlsx.index + 1) # Setze nachträglich Index-Spalte des Typs int\n",
    "    return df_xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _import_xlsx_inkl_sonderZ_Ber(tabelle):\n",
    "    '''\n",
    "    Automatische Bereinigung auf Sonderzeichen in den genannten Spalten (Loop) in dictSpaltenZurAnpassung\n",
    "\n",
    "    INPUT:\n",
    "        Tabellenname (.xlsx)\n",
    "    OUTPUT:\n",
    "        Datenframe inkl. bereinigter Spalten\n",
    "    \n",
    "    '''\n",
    "    df_xlsx = _import_xlsx_conv_to_df(tabelle)\n",
    "    \n",
    "    df_xlsx = df_xlsx.rename(columns = {c: c.replace(' ','') for c in df_xlsx.columns})\n",
    "    \n",
    "    \n",
    "    if tabelle in dictSpaltenZurAnpassung:\n",
    "        for spalte in range(0,len(dictSpaltenZurAnpassung[tabelle])):\n",
    "            spl = dictSpaltenZurAnpassung[tabelle][spalte]\n",
    "            df_xlsx[spl] = df_xlsx[spl].str.replace(r'\\W','')\n",
    "            print(spl)\n",
    "        \n",
    "        return df_xlsx\n",
    "            \n",
    "    else:\n",
    "        print(\"Tabelle {} enthält keine zu \\nbereinigenden Spalten.\".format(tabelle))\n",
    "        #continue\n",
    "\n",
    "    return df_xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fill_xlsx_in_ms_sql(tabelle):\n",
    "    \"\"\"\n",
    "    Einlesen xlsx in Vorlauf_DB Datenbank tabs\n",
    "\n",
    "    INPUT:\n",
    "        Tabellenname:\n",
    "        xlsx:   Covin Validierungsregeln \n",
    "                Lieferanten\n",
    "                Artikel Zuordnung agka class\n",
    "                agka class\n",
    "                Überleitung agka zu eClass\n",
    "                Individueller Warenkorb\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    chunk_size = 10000\n",
    "    start_time = time.time() \n",
    "    server_verbindung = _verbinde_ms_sql()\n",
    "    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        df_xlsx = _import_xlsx_inkl_sonderZ_Ber(tabelle) # Übergabe des importierten, bereinigten DF\n",
    "        if tabelle == \"tbl_lieferanten_master\":\n",
    "            df_xlsx_lief = df_xlsx[['LieferantenNr'\n",
    "                                    ,'AGKALieferanten_Lieferant'\n",
    "                                    ,'MiiLieferantGlobal'\n",
    "                                    ,'AGKAMED_Langname'\n",
    "                                    ,'Finale_UStID'\n",
    "                                    ,'tbl_index']]\n",
    "#            df_xlsx_lief = df_xlsx_lief.assign(tbl_index=df_xlsx.index + 1) # Setze nachträglich Index-Spalte des Typs int\n",
    "            df_xlsx_lief = df_xlsx_lief.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "            df_xlsx_lief_kat = df_xlsx[['TPSHORTNAME'\n",
    "                                        ,'LieferantenNr'\n",
    "                                        ,'tbl_index']]\n",
    "            df_xlsx_lief.to_sql('tbl_lieferanten',con=server_verbindung,if_exists='replace',index=False)\n",
    "            df_xlsx_lief_kat.to_sql('tbl_lieferanten_kataloge',con=server_verbindung,if_exists='replace',index=False)\n",
    "\n",
    "        elif tabelle == 'tbl_artikel_agkaClass_jd':\n",
    "            df_xlsx['_artikel_key_class_'] = ''\n",
    "            df_xlsx.to_sql('tbl_artikel_agkaClass_jd',con=server_verbindung,if_exists='replace',index=False)\n",
    "\n",
    "        else:\n",
    "            df_xlsx.to_sql(tabelle,con=server_verbindung,if_exists='replace',index=False)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Lösche verarbeitete Ordner mit csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_csv_fold_for_deletion(pfad):\n",
    "    \"\"\"\n",
    "    Listet entpackte Ordner mit csv-Dateien.\n",
    "    \n",
    "    INPUT:\n",
    "        pfad: Pfad aus welchem Ordner gelöscht werden sollen. Hier i. d. R. ghx_cataloge_path.\n",
    "    OUTPUT:\n",
    "        lst_for_del: Liste der Ordner in Arbeitsordner\n",
    "    \"\"\"\n",
    "    \n",
    "    pfad = next(os.walk(pfad))[0]\n",
    "    ordner_in_ftp =  next(os.walk(pfad))[1]\n",
    "    lst_for_del = [pfad + ordner for ordner in ordner_in_ftp]\n",
    "    return lst_for_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _del_fold_csv(pfad):\n",
    "    \"\"\"\n",
    "    Löscht durch _find_csv_fold_for_deletion erkannte Ordner aus Arbeitsordner.\n",
    "    \n",
    "    INPUT:\n",
    "        lst_for_del: Ausführung der Funktion zur Bestimmung der zu löschenden Ordner.\n",
    "        pfad: Pfad aus welchem Ordner gelöscht werden sollen. Hier i. d. R. ghx_cataloge_path.\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    lst_for_del = _find_csv_fold_for_deletion(pfad)\n",
    "    if not lst_for_del:\n",
    "        print(\"Keine csv-Ordner in Arbeitsordner vorhanden.\"\n",
    "             \"\\nFunktion wird beendet.\")\n",
    "        return\n",
    "    else:\n",
    "        try:\n",
    "            [shutil.rmtree(fold) for fold in lst_for_del]\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Skripte Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _verbinde_zu_server_und_db():\n",
    "    with pyodbc.connect(r\"DRIVER={SQL Server Native Client 11.0};\"\n",
    "                                \"SERVER=192.168.16.124;\"\n",
    "                                \"DATABASE=Vorlauf_DB;\"\n",
    "                                \"Trusted_Connection=yes;\") as verb_db:\n",
    "\n",
    "        return verb_db\n",
    "#                                \"SERVER=192.168.16.124;\"\n",
    "#                                \"DATABASE=Vorlauf_DB;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sql_executer(sql_string):\n",
    "    db_verb = _verbinde_zu_server_und_db()\n",
    "#     db_verb = _verbinde_ms_sql()\n",
    "    cur = db_verb.cursor()\n",
    "    cur.execute(sql_string)\n",
    "    db_verb.commit()\n",
    "\n",
    "#     Vorlage: Alternativer sql_executer über sqlalchemy \n",
    "#     with _verbinde_ms_sql().connect() as connection:\n",
    "#         result = connection.execute(sql_delete_empty_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _export_warenkorb():\n",
    "    \"\"\"\n",
    "    Importiert einen Warenkorb in die Datenbank.\n",
    "    \n",
    "    INPUT:\n",
    "       Funktion _fill_xlsx_in_ms_sql für Tabelle Warenkorb\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    start = time.process_time()\n",
    "    \n",
    "    _fill_xlsx_in_ms_sql('tbl_warenkorb') # Import Warenkorb in DB \n",
    "    _sql_executer(sql_export_warenkorb) # Ausführung Query Mapping Warenkorb mit tbl_kataloge_roh\n",
    "    db_verb = _verbinde_zu_server_und_db() \n",
    "#    df_exp_xlsx = pd.read_sql(sql_export_warenkorb, db_verb) # Funktioniert bei kleinen Tabellen \n",
    "\n",
    "    df_exp_xlsx = pd.DataFrame()\n",
    "    for df_teil in pd.read_sql(sql_export_warenkorb,db_verb, chunksize = 3):\n",
    "        df_exp_xlsx = df_exp_xlsx.append(df_teil,ignore_index=True)\n",
    "    # Nachträgliche Anpassungen des Warenkorbs \n",
    "    df_exp_xlsx.drop_duplicates(subset=['Index'\n",
    "                                        ,'MFRNAME'\n",
    "                                        ,'MFRPARTNUM'\n",
    "                                        ,'SUPPLIERNAME'\n",
    "                                        ,'SUPPLIERPARTNUM'\n",
    "                                        ,'BaseUOM'\n",
    "                                        ,'NOU'\n",
    "                                        ,'UOM' \n",
    "                                        ,'_prio_flag_' \n",
    "                                        ,'_artikel_key_'], inplace=True) # keep='false' \n",
    "#     df_exp_xlsx.query('_prio_flag_ != \"0\"',inplace = True)\n",
    "    df_exp_xlsx.to_excel(\"KATALOGMAPPING_eCl@ss.xlsx\") # Export Warenkorb in xlsx\n",
    "    print('\\nWarenkorb Mapping abgeschlossen.\\ndie KATALOGMAPPING_eCl@ss.xlsx Datei liegt bereit.')\n",
    "    print(time.process_time() - start)\n",
    "    db_verb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _export_warenkorb_gtin():\n",
    "    \"\"\"\n",
    "    Importiert einen Warenkorb in die Datenbank.\n",
    "    \n",
    "    INPUT:\n",
    "       Funktion _fill_xlsx_in_ms_sql für Tabelle Warenkorb\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    start = time.process_time()\n",
    "    \n",
    "    _fill_xlsx_in_ms_sql('tbl_warenkorb') # Import Warenkorb in DB \n",
    "    _sql_executer(sql_export_warenkorb_gtin) # Ausführung Query Mapping Warenkorb mit tbl_kataloge_roh\n",
    "    db_verb = _verbinde_zu_server_und_db() \n",
    "#    df_exp_xlsx = pd.read_sql(sql_export_warenkorb, db_verb) # Funktioniert bei kleinen Tabellen \n",
    "\n",
    "    df_exp_xlsx = pd.DataFrame()\n",
    "    for df_teil in pd.read_sql(sql_export_warenkorb_gtin,db_verb, chunksize = 3):\n",
    "        df_exp_xlsx = df_exp_xlsx.append(df_teil,ignore_index=True)\n",
    "    # Nachträgliche Anpassungen des Warenkorbs \n",
    "    df_exp_xlsx.drop_duplicates(subset=['Index'\n",
    "                                        ,'MFRNAME'\n",
    "                                        ,'MFRPARTNUM'\n",
    "                                        ,'SUPPLIERNAME'\n",
    "                                        ,'SUPPLIERPARTNUM'\n",
    "                                        ,'BaseUOM'\n",
    "                                        ,'NOU'\n",
    "                                        ,'UOM' \n",
    "                                        ,'_prio_flag_' \n",
    "                                        ,'_artikel_key_'], inplace=True) # keep='false' \n",
    "#     df_exp_xlsx.query('_prio_flag_ != \"0\"',inplace = True)\n",
    "    df_exp_xlsx.to_excel(\"KATALOGMAPPING_eCl@ss.xlsx\") # Export Warenkorb in xlsx\n",
    "    print('\\nWarenkorb Mapping abgeschlossen.\\ndie KATALOGMAPPING_eCl@ss.xlsx Datei liegt bereit.')\n",
    "    print(time.process_time() - start)\n",
    "    db_verb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Vorlauf_DB].[dbo].tbl_artikel_agkaClass_jd'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_tabellen = [\"[Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\"\n",
    "               ,\"[Vorlauf_DB].[dbo].[tbl_verw_inload_kataloge]\"\n",
    "               ,\"[Vorlauf_DB].[dbo].tbl_verw_csv\"\n",
    "               ,\"[Vorlauf_DB].[dbo].tbl_verw_zip\"\n",
    "               ,\"[Vorlauf_DB].[dbo].tbl_verw_ean\"\n",
    "               ,\"[Vorlauf_DB].[dbo].tbl_pzn_lauertaxe\"\n",
    "               ,\"[Vorlauf_DB].[dbo].tbl_lieferanten\"\n",
    "               ,\"[Vorlauf_DB].[dbo].tbl_artikel_agkaClass_jd\"]\n",
    "db_tabellen[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warenkorb: Mapping GTIN Ebene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_export_warenkorb_gtin = \"\"\"\n",
    "WITH cte_warenkorb_map AS (\n",
    "SELECT [Index]\n",
    ",[LieferantenNrAGKA]\n",
    ",[LieferantenNameGlobal]\n",
    ",[LieferantenArtikelNr]\n",
    ",[LieferantenArtikelBeschreibung]\n",
    ",[NOU]\n",
    ",[UOM]\n",
    ",CONCAT(LieferantenNameGlobal,'°',LieferantenArtikelNr,'°',[UOM],'°',[NOU]) AS ARTIKEL_KEY_WARENKORB\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_warenkorb]\n",
    "),\n",
    "ghx_kompl AS (\n",
    "SELECT cte_warenkorb_map.[Index]\n",
    "      ,cte_warenkorb_map.ARTIKEL_KEY_WARENKORB\n",
    "      ,[TPSHORTNAME]\n",
    "      ,[ACTIONCODE]\n",
    "      ,[LOCALE]\n",
    "      ,[MFRNAME]\n",
    "      ,[MFRPARTNUM]\n",
    "      ,[SUPPLIERNAME]\n",
    "      ,[SUPPLIERPARTNUM]\n",
    "      ,[BaseUOM]\n",
    "      ,tbl_roh.[NOU]\n",
    "      ,tbl_roh.[UOM]\n",
    "      ,[SHORTDESC]\n",
    "      ,[LISTPRICE]\n",
    "      ,[TAXWARECOMMODITYCODE]\n",
    "      ,[LONGDESC]\n",
    "      ,[SEARCHABLEKEYWORD]\n",
    "      ,[EUROPMEDICALDEVICEDIRECTIVECLASS]\n",
    "      ,[PACKAGINGINFORMATION]\n",
    "      ,[EAN]\n",
    "      ,[PZN]\n",
    "      ,[LOTSIZE]\n",
    "      ,[MINORDER]\n",
    "      ,[MAXORDER]\n",
    "      ,[LEADTIME]\n",
    "      ,[AVAILABILITY]\n",
    "      ,[NETWEIGHT]\n",
    "      ,[GROSSWEIGHT]\n",
    "      ,[WEIGHTUOM]\n",
    "      ,[PRODUCTWIDTH]\n",
    "      ,[PRODUCTHEIGHT]\n",
    "      ,[PRODUCTDEPTH]\n",
    "      ,[PACKAGEDWIDTH]\n",
    "      ,[PACKAGEDHEIGHT]\n",
    "      ,[PACKAGEDDEPTH]\n",
    "      ,[DIMENSIONUOM]\n",
    "      ,[CATLEVEL1]\n",
    "      ,[CATLEVEL2]\n",
    "      ,[CATLEVEL3]\n",
    "      ,[CATLEVEL4]\n",
    "      ,[CATLEVEL5]\n",
    "      ,[CATNAME]\n",
    "      ,[CATVERSION]\n",
    "      ,[CATCODE]\n",
    "      ,[OTHERPARTNUMNAME1]\n",
    "      ,[OTHERPARTNUM1]\n",
    "      ,[OTHERPARTNUMNAME2]\n",
    "      ,[OTHERPARTNUM2]\n",
    "      ,[DIVISION]\n",
    "      ,[HAZMATFLAG]\n",
    "      ,[RECYCLEDMATERIALS]\n",
    "      ,[SAFETYFLAG]\n",
    "      ,[RESTRICTFLAG]\n",
    "      ,[RESTRICTTYPES]\n",
    "      ,[PURPOSE1]\n",
    "      ,[LOCATION1]\n",
    "      ,[TYPE1]\n",
    "      ,[PRIORITY1]\n",
    "      ,[PURPOSE2]\n",
    "      ,[LOCATION2]\n",
    "      ,[TYPE2]\n",
    "      ,[PRIORITY2]\n",
    "      ,[PURPOSE3]\n",
    "      ,[LOCATION3]\n",
    "      ,[TYPE3]\n",
    "      ,[PRIORITY3]\n",
    "      ,[PURPOSE4]\n",
    "      ,[LOCATION4]\n",
    "      ,[TYPE4]\n",
    "      ,[PRIORITY4]\n",
    "      ,[PURPOSE5]\n",
    "      ,[LOCATION5]\n",
    "      ,[TYPE5]\n",
    "      ,[PRIORITY5]\n",
    "      ,[PURPOSE6]\n",
    "      ,[LOCATION6]\n",
    "      ,[TYPE6]\n",
    "      ,[PRIORITY6]\n",
    "      ,[PURPOSE7]\n",
    "      ,[LOCATION7]\n",
    "      ,[TYPE7]\n",
    "      ,[PRIORITY7]\n",
    "      ,[PURPOSE8]\n",
    "      ,[LOCATION8]\n",
    "      ,[TYPE8]\n",
    "      ,[PRIORITY8]\n",
    "      ,[PURPOSE9]\n",
    "      ,[LOCATION9]\n",
    "      ,[TYPE9]\n",
    "      ,[PRIORITY9]\n",
    "      ,[PURPOSE10]\n",
    "      ,[LOCATION10]\n",
    "      ,[TYPE10]\n",
    "      ,[PRIORITY10]\n",
    "      ,[SUPPLIERNAMEOLD]\n",
    "      ,[SUPPLIERPARTNUMOLD]\n",
    "      ,[UOMOLD]\n",
    "      ,[AttributeData]\n",
    "      ,[_computed_normalized_suppliername_]\n",
    "      ,[_katalog_inload_]\n",
    "      ,[_computed_normalized_art_nr_]\n",
    "      ,[_artikel_key_inload_]\n",
    "      ,[_csv_file_]\n",
    "      ,[_prio_flag_]\n",
    "      ,[_hibc_]\n",
    "      ,[_gtin_]\n",
    "      ,[_ean_kommentar_]\n",
    "      ,[_artikel_key_]\n",
    "          ,[_LieferantenNameGlobal_]\n",
    "          ,[_AGKA_Kreditoren_ID_]\n",
    "          ,[_artikel_key_class_]\n",
    "          ,[_agka_class_id_]\n",
    "          ,[_UStID_Vorschlag_]\n",
    "          ,[_gtin_pruefziffer_]\n",
    "          ,[_gtin_pruefung_kommentar_]\n",
    "\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh] tbl_roh\n",
    "JOIN cte_warenkorb_map \n",
    "ON tbl_roh._artikel_key_ = cte_warenkorb_map.ARTIKEL_KEY_WARENKORB\n",
    "WHERE tbl_roh.[_prio_flag_] NOT LIKE '0'\n",
    "AND tbl_roh.[_LieferantenNameGlobal_] = cte_warenkorb_map.[LieferantenNameGlobal]\n",
    ")\n",
    "SELECT \n",
    "    tbl_war.[Index]\n",
    "    ,[LieferantenNrAGKA]\n",
    "    ,[LieferantenNameGlobal]\n",
    "    ,[LieferantenArtikelNr]\n",
    "    ,[LieferantenArtikelBeschreibung]\n",
    "    ,ghx_kompl.[NOU]\n",
    "    ,ghx_kompl.[UOM]\n",
    "    ,ghx_kompl.ARTIKEL_KEY_WARENKORB\n",
    "    ,[TPSHORTNAME]\n",
    "    ,[ACTIONCODE]\n",
    "    ,[LOCALE]\n",
    "    ,[MFRNAME]\n",
    "    ,[MFRPARTNUM]\n",
    "    ,[SUPPLIERNAME]\n",
    "    ,[SUPPLIERPARTNUM]\n",
    "    ,[BaseUOM]\n",
    "    ,ghx_kompl.[NOU]\n",
    "    ,ghx_kompl.[UOM]\n",
    "    ,[SHORTDESC]\n",
    "    ,[LISTPRICE]\n",
    "    ,[TAXWARECOMMODITYCODE]\n",
    "    ,[LONGDESC]\n",
    "    ,[SEARCHABLEKEYWORD]\n",
    "    ,[EUROPMEDICALDEVICEDIRECTIVECLASS]\n",
    "    ,[PACKAGINGINFORMATION]\n",
    "    ,[EAN]\n",
    "    ,[PZN]\n",
    "    ,[LOTSIZE]\n",
    "    ,[MINORDER]\n",
    "    ,[MAXORDER]\n",
    "    ,[LEADTIME]\n",
    "    ,[AVAILABILITY]\n",
    "    ,[NETWEIGHT]\n",
    "    ,[GROSSWEIGHT]\n",
    "    ,[WEIGHTUOM]\n",
    "    ,[PRODUCTWIDTH]\n",
    "    ,[PRODUCTHEIGHT]\n",
    "    ,[PRODUCTDEPTH]\n",
    "    ,[PACKAGEDWIDTH]\n",
    "    ,[PACKAGEDHEIGHT]\n",
    "    ,[PACKAGEDDEPTH]\n",
    "    ,[DIMENSIONUOM]\n",
    "    ,[CATLEVEL1]\n",
    "    ,[CATLEVEL2]\n",
    "    ,[CATLEVEL3]\n",
    "    ,[CATLEVEL4]\n",
    "    ,[CATLEVEL5]\n",
    "    ,[CATNAME]\n",
    "    ,[CATVERSION]\n",
    "    ,[CATCODE]\n",
    "    ,[OTHERPARTNUMNAME1]\n",
    "    ,[OTHERPARTNUM1]\n",
    "    ,[OTHERPARTNUMNAME2]\n",
    "    ,[OTHERPARTNUM2]\n",
    "    ,[DIVISION]\n",
    "    ,[HAZMATFLAG]\n",
    "    ,[RECYCLEDMATERIALS]\n",
    "    ,[SAFETYFLAG]\n",
    "    ,[RESTRICTFLAG]\n",
    "    ,[RESTRICTTYPES]\n",
    "    ,[PURPOSE1]\n",
    "    ,[LOCATION1]\n",
    "    ,[TYPE1]\n",
    "    ,[PRIORITY1]\n",
    "    ,[PURPOSE2]\n",
    "    ,[LOCATION2]\n",
    "    ,[TYPE2]\n",
    "    ,[PRIORITY2]\n",
    "    ,[PURPOSE3]\n",
    "    ,[LOCATION3]\n",
    "    ,[TYPE3]\n",
    "    ,[PRIORITY3]\n",
    "    ,[PURPOSE4]\n",
    "    ,[LOCATION4]\n",
    "    ,[TYPE4]\n",
    "    ,[PRIORITY4]\n",
    "    ,[PURPOSE5]\n",
    "    ,[LOCATION5]\n",
    "    ,[TYPE5]\n",
    "    ,[PRIORITY5]\n",
    "    ,[PURPOSE6]\n",
    "    ,[LOCATION6]\n",
    "    ,[TYPE6]\n",
    "    ,[PRIORITY6]\n",
    "    ,[PURPOSE7]\n",
    "    ,[LOCATION7]\n",
    "    ,[TYPE7]\n",
    "    ,[PRIORITY7]\n",
    "    ,[PURPOSE8]\n",
    "    ,[LOCATION8]\n",
    "    ,[TYPE8]\n",
    "    ,[PRIORITY8]\n",
    "    ,[PURPOSE9]\n",
    "    ,[LOCATION9]\n",
    "    ,[TYPE9]\n",
    "    ,[PRIORITY9]\n",
    "    ,[PURPOSE10]\n",
    "    ,[LOCATION10]\n",
    "    ,[TYPE10]\n",
    "    ,[PRIORITY10]\n",
    "    ,[SUPPLIERNAMEOLD]\n",
    "    ,[SUPPLIERPARTNUMOLD]\n",
    "    ,[UOMOLD]\n",
    "    ,[AttributeData]\n",
    "    ,[_computed_normalized_suppliername_]\n",
    "    ,[_katalog_inload_]\n",
    "    ,[_computed_normalized_art_nr_]\n",
    "    ,[_artikel_key_inload_]\n",
    "    ,[_csv_file_]\n",
    "    ,[_prio_flag_]\n",
    "    ,[_hibc_]\n",
    "    ,[_gtin_]\n",
    "    ,[_ean_kommentar_]\n",
    "    ,[_artikel_key_]\n",
    "      ,[_LieferantenNameGlobal_]\n",
    "      ,[_artikel_key_]\n",
    "      ,[_AGKA_Kreditoren_ID_]\n",
    "      ,[_artikel_key_class_]\n",
    "      ,[_agka_class_id_]\n",
    "      ,[_UStID_Vorschlag_]\n",
    "      ,[_gtin_pruefziffer_]\n",
    "      ,[_gtin_pruefung_kommentar_]\n",
    "\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_warenkorb] tbl_war\n",
    "LEFT JOIN  ghx_kompl\n",
    "ON   ghx_kompl.[Index] = tbl_war.[Index]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warenkorb: Mapping eCl@ss Ebene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_export_warenkorb = \"\"\"\n",
    "WITH cte_warenkorb_map AS (\n",
    "SELECT [Index]\n",
    ",[LieferantenNrAGKA]\n",
    ",[LieferantenNameGlobal]\n",
    ",[LieferantenArtikelNr]\n",
    ",[LieferantenArtikelBeschreibung]\n",
    ",[NOU]\n",
    ",[UOM]\n",
    ",CONCAT(LieferantenNameGlobal,'°',LieferantenArtikelNr) AS ARTIKEL_KEY_WARENKORB\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_warenkorb]\n",
    "),\n",
    "ghx_kompl AS (\n",
    "SELECT cte_warenkorb_map.[Index]\n",
    "      ,cte_warenkorb_map.ARTIKEL_KEY_WARENKORB\n",
    "      ,[TPSHORTNAME]\n",
    "      ,[ACTIONCODE]\n",
    "      ,[LOCALE]\n",
    "      ,[MFRNAME]\n",
    "      ,[MFRPARTNUM]\n",
    "      ,[SUPPLIERNAME]\n",
    "      ,[SUPPLIERPARTNUM]\n",
    "      ,[BaseUOM]\n",
    "      ,tbl_roh.[NOU]\n",
    "      ,tbl_roh.[UOM]\n",
    "      ,[SHORTDESC]\n",
    "      ,[LISTPRICE]\n",
    "      ,[TAXWARECOMMODITYCODE]\n",
    "      ,[LONGDESC]\n",
    "      ,[SEARCHABLEKEYWORD]\n",
    "      ,[EUROPMEDICALDEVICEDIRECTIVECLASS]\n",
    "      ,[PACKAGINGINFORMATION]\n",
    "      ,[EAN]\n",
    "      ,[PZN]\n",
    "      ,[LOTSIZE]\n",
    "      ,[MINORDER]\n",
    "      ,[MAXORDER]\n",
    "      ,[LEADTIME]\n",
    "      ,[AVAILABILITY]\n",
    "      ,[NETWEIGHT]\n",
    "      ,[GROSSWEIGHT]\n",
    "      ,[WEIGHTUOM]\n",
    "      ,[PRODUCTWIDTH]\n",
    "      ,[PRODUCTHEIGHT]\n",
    "      ,[PRODUCTDEPTH]\n",
    "      ,[PACKAGEDWIDTH]\n",
    "      ,[PACKAGEDHEIGHT]\n",
    "      ,[PACKAGEDDEPTH]\n",
    "      ,[DIMENSIONUOM]\n",
    "      ,[CATLEVEL1]\n",
    "      ,[CATLEVEL2]\n",
    "      ,[CATLEVEL3]\n",
    "      ,[CATLEVEL4]\n",
    "      ,[CATLEVEL5]\n",
    "      ,[CATNAME]\n",
    "      ,[CATVERSION]\n",
    "      ,[CATCODE]\n",
    "      ,[OTHERPARTNUMNAME1]\n",
    "      ,[OTHERPARTNUM1]\n",
    "      ,[OTHERPARTNUMNAME2]\n",
    "      ,[OTHERPARTNUM2]\n",
    "      ,[DIVISION]\n",
    "      ,[HAZMATFLAG]\n",
    "      ,[RECYCLEDMATERIALS]\n",
    "      ,[SAFETYFLAG]\n",
    "      ,[RESTRICTFLAG]\n",
    "      ,[RESTRICTTYPES]\n",
    "      ,[PURPOSE1]\n",
    "      ,[LOCATION1]\n",
    "      ,[TYPE1]\n",
    "      ,[PRIORITY1]\n",
    "      ,[PURPOSE2]\n",
    "      ,[LOCATION2]\n",
    "      ,[TYPE2]\n",
    "      ,[PRIORITY2]\n",
    "      ,[PURPOSE3]\n",
    "      ,[LOCATION3]\n",
    "      ,[TYPE3]\n",
    "      ,[PRIORITY3]\n",
    "      ,[PURPOSE4]\n",
    "      ,[LOCATION4]\n",
    "      ,[TYPE4]\n",
    "      ,[PRIORITY4]\n",
    "      ,[PURPOSE5]\n",
    "      ,[LOCATION5]\n",
    "      ,[TYPE5]\n",
    "      ,[PRIORITY5]\n",
    "      ,[PURPOSE6]\n",
    "      ,[LOCATION6]\n",
    "      ,[TYPE6]\n",
    "      ,[PRIORITY6]\n",
    "      ,[PURPOSE7]\n",
    "      ,[LOCATION7]\n",
    "      ,[TYPE7]\n",
    "      ,[PRIORITY7]\n",
    "      ,[PURPOSE8]\n",
    "      ,[LOCATION8]\n",
    "      ,[TYPE8]\n",
    "      ,[PRIORITY8]\n",
    "      ,[PURPOSE9]\n",
    "      ,[LOCATION9]\n",
    "      ,[TYPE9]\n",
    "      ,[PRIORITY9]\n",
    "      ,[PURPOSE10]\n",
    "      ,[LOCATION10]\n",
    "      ,[TYPE10]\n",
    "      ,[PRIORITY10]\n",
    "      ,[SUPPLIERNAMEOLD]\n",
    "      ,[SUPPLIERPARTNUMOLD]\n",
    "      ,[UOMOLD]\n",
    "      ,[AttributeData]\n",
    "      ,[_computed_normalized_suppliername_]\n",
    "      ,[_katalog_inload_]\n",
    "      ,[_computed_normalized_art_nr_]\n",
    "      ,[_artikel_key_inload_]\n",
    "      ,[_csv_file_]\n",
    "      ,[_prio_flag_]\n",
    "      ,[_hibc_]\n",
    "      ,[_gtin_]\n",
    "      ,[_ean_kommentar_]\n",
    "      ,[_artikel_key_]\n",
    "      ,[_LieferantenNameGlobal_]\n",
    "      ,[_AGKA_Kreditoren_ID_]\n",
    "      ,[_artikel_key_class_]\n",
    "      ,[_agka_class_id_]\n",
    "      ,[_UStID_Vorschlag_]\n",
    "      ,[_gtin_pruefziffer_]\n",
    "      ,[_gtin_pruefung_kommentar_]\n",
    "      ,[_AGKclassCode-LEVEL1_]\n",
    "      ,[_AGKclassCode-LEVEL2_]\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh] tbl_roh\n",
    "JOIN cte_warenkorb_map \n",
    "ON CONCAT(tbl_roh.[_LieferantenNameGlobal_],'°',tbl_roh._computed_normalized_art_nr_) = cte_warenkorb_map.ARTIKEL_KEY_WARENKORB\n",
    "WHERE tbl_roh.[_prio_flag_] NOT LIKE '0'\n",
    "AND tbl_roh.[_LieferantenNameGlobal_] = cte_warenkorb_map.[LieferantenNameGlobal]\n",
    ")\n",
    "SELECT \n",
    "    tbl_war.[Index]\n",
    "    ,[LieferantenNrAGKA]\n",
    "    ,[LieferantenNameGlobal]\n",
    "    ,[LieferantenArtikelNr]\n",
    "    ,[LieferantenArtikelBeschreibung]\n",
    "    ,ghx_kompl.[NOU]\n",
    "    ,ghx_kompl.[UOM]\n",
    "    ,ghx_kompl.ARTIKEL_KEY_WARENKORB\n",
    "    ,[TPSHORTNAME]\n",
    "    ,[ACTIONCODE]\n",
    "    ,[LOCALE]\n",
    "    ,[MFRNAME]\n",
    "    ,[MFRPARTNUM]\n",
    "    ,[SUPPLIERNAME]\n",
    "    ,[SUPPLIERPARTNUM]\n",
    "    ,[BaseUOM]\n",
    "    ,ghx_kompl.[NOU]\n",
    "    ,ghx_kompl.[UOM]\n",
    "    ,[SHORTDESC]\n",
    "    ,[LISTPRICE]\n",
    "    ,[TAXWARECOMMODITYCODE]\n",
    "    ,[LONGDESC]\n",
    "    ,[SEARCHABLEKEYWORD]\n",
    "    ,[EUROPMEDICALDEVICEDIRECTIVECLASS]\n",
    "    ,[PACKAGINGINFORMATION]\n",
    "    ,[EAN]\n",
    "    ,[PZN]\n",
    "    ,[LOTSIZE]\n",
    "    ,[MINORDER]\n",
    "    ,[MAXORDER]\n",
    "    ,[LEADTIME]\n",
    "    ,[AVAILABILITY]\n",
    "    ,[NETWEIGHT]\n",
    "    ,[GROSSWEIGHT]\n",
    "    ,[WEIGHTUOM]\n",
    "    ,[PRODUCTWIDTH]\n",
    "    ,[PRODUCTHEIGHT]\n",
    "    ,[PRODUCTDEPTH]\n",
    "    ,[PACKAGEDWIDTH]\n",
    "    ,[PACKAGEDHEIGHT]\n",
    "    ,[PACKAGEDDEPTH]\n",
    "    ,[DIMENSIONUOM]\n",
    "    ,[CATLEVEL1]\n",
    "    ,[CATLEVEL2]\n",
    "    ,[CATLEVEL3]\n",
    "    ,[CATLEVEL4]\n",
    "    ,[CATLEVEL5]\n",
    "    ,[CATNAME]\n",
    "    ,[CATVERSION]\n",
    "    ,[CATCODE]\n",
    "    ,[OTHERPARTNUMNAME1]\n",
    "    ,[OTHERPARTNUM1]\n",
    "    ,[OTHERPARTNUMNAME2]\n",
    "    ,[OTHERPARTNUM2]\n",
    "    ,[DIVISION]\n",
    "    ,[HAZMATFLAG]\n",
    "    ,[RECYCLEDMATERIALS]\n",
    "    ,[SAFETYFLAG]\n",
    "    ,[RESTRICTFLAG]\n",
    "    ,[RESTRICTTYPES]\n",
    "    ,[PURPOSE1]\n",
    "    ,[LOCATION1]\n",
    "    ,[TYPE1]\n",
    "    ,[PRIORITY1]\n",
    "    ,[PURPOSE2]\n",
    "    ,[LOCATION2]\n",
    "    ,[TYPE2]\n",
    "    ,[PRIORITY2]\n",
    "    ,[PURPOSE3]\n",
    "    ,[LOCATION3]\n",
    "    ,[TYPE3]\n",
    "    ,[PRIORITY3]\n",
    "    ,[PURPOSE4]\n",
    "    ,[LOCATION4]\n",
    "    ,[TYPE4]\n",
    "    ,[PRIORITY4]\n",
    "    ,[PURPOSE5]\n",
    "    ,[LOCATION5]\n",
    "    ,[TYPE5]\n",
    "    ,[PRIORITY5]\n",
    "    ,[PURPOSE6]\n",
    "    ,[LOCATION6]\n",
    "    ,[TYPE6]\n",
    "    ,[PRIORITY6]\n",
    "    ,[PURPOSE7]\n",
    "    ,[LOCATION7]\n",
    "    ,[TYPE7]\n",
    "    ,[PRIORITY7]\n",
    "    ,[PURPOSE8]\n",
    "    ,[LOCATION8]\n",
    "    ,[TYPE8]\n",
    "    ,[PRIORITY8]\n",
    "    ,[PURPOSE9]\n",
    "    ,[LOCATION9]\n",
    "    ,[TYPE9]\n",
    "    ,[PRIORITY9]\n",
    "    ,[PURPOSE10]\n",
    "    ,[LOCATION10]\n",
    "    ,[TYPE10]\n",
    "    ,[PRIORITY10]\n",
    "    ,[SUPPLIERNAMEOLD]\n",
    "    ,[SUPPLIERPARTNUMOLD]\n",
    "    ,[UOMOLD]\n",
    "    ,[AttributeData]\n",
    "    ,[_computed_normalized_suppliername_]\n",
    "    ,[_katalog_inload_]\n",
    "    ,[_computed_normalized_art_nr_]\n",
    "    ,[_artikel_key_inload_]\n",
    "    ,[_csv_file_]\n",
    "    ,[_prio_flag_]\n",
    "    ,[_hibc_]\n",
    "    ,[_gtin_]\n",
    "    ,[_ean_kommentar_]\n",
    "    ,[_artikel_key_]\n",
    "      ,[_LieferantenNameGlobal_]\n",
    "      ,[_AGKA_Kreditoren_ID_]\n",
    "      ,[_artikel_key_class_]\n",
    "      ,[_agka_class_id_]\n",
    "      ,[_UStID_Vorschlag_]\n",
    "      ,[_gtin_pruefziffer_]\n",
    "      ,[_gtin_pruefung_kommentar_]\n",
    "      ,[_AGKclassCode-LEVEL1_]\n",
    "      ,[_AGKclassCode-LEVEL2_]\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_warenkorb] tbl_war\n",
    "LEFT JOIN  ghx_kompl\n",
    "ON   ghx_kompl.[Index] = tbl_war.[Index]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_set_import_flag = \"\"\"   with cte AS (\n",
    "                            SELECT COUNT(*) Anz_Datensaetze\n",
    "                            ,[Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_artikel_key_inload_]\n",
    "                            FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "                            GROUP BY [_artikel_key_inload_]),\n",
    "                            cte2 AS (\n",
    "                            SELECT \n",
    "                            [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_inload_  \n",
    "                            ,_aktuellster_inload_ = MAX(_katalog_inload_)\n",
    "                            FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "                            GROUP BY _artikel_key_inload_),\n",
    "                            cte_master AS (\n",
    "                            SELECT  [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_inload_\n",
    "                            ,[Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._katalog_inload_\n",
    "                            ,cte.Anz_Datensaetze\n",
    "                            ,CASE WHEN cte.Anz_Datensaetze > 1 \n",
    "                            AND [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._katalog_inload_ = cte2._aktuellster_inload_\n",
    "                            THEN '1' \n",
    "                            WHEN cte.Anz_Datensaetze = 1 THEN '1'\n",
    "                            ELSE '0' END AS _prio_flag_\t\n",
    "                            FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "                            Left JOIN cte\n",
    "                            ON cte._artikel_key_inload_ = [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_inload_\n",
    "                            left JOIN cte2 \n",
    "                            ON cte2._artikel_key_inload_ = [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_inload_)\n",
    "                            \n",
    "                            UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "                            SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._prio_flag_ = cte_master._prio_flag_\n",
    "                            FROM cte_master\n",
    "                            LEFT JOIN [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "                            ON [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_inload_= cte_master._artikel_key_inload_\n",
    "                            AND [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._katalog_inload_ = cte_master._katalog_inload_\n",
    "                            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösche leere Zeilen\n",
    "sql_delete_empty_rows = \"\"\"\n",
    "                        DELETE FROM {}\n",
    "                        WHERE \n",
    "                                  [TPSHORTNAME] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [ACTIONCODE] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [MFRNAME] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [MFRPARTNUM] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [SUPPLIERNAME] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [SUPPLIERPARTNUM] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [BaseUOM] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [NOU] IS NULL OR [TPSHORTNAME] = ''\n",
    "                              AND [UOM] IS NULL OR [TPSHORTNAME] = ''\n",
    "                        \"\"\".format(db_tabellen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ean_handling_master = \"\"\"\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "set _hibc_ = NULL\n",
    "    ,_gtin_ = NULL\n",
    "    ,_gtin_pruefung_kommentar_ = NULL\n",
    "    ,_ean_kommentar_ = NULL\n",
    "    ,_gtin_pruefziffer_ = NULL\n",
    "\t\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "set _gtin_ = LTRIM(RTRIM(EAN))\n",
    "from [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "where LTRIM(RTRIM(EAN)) IS NOT NULL\n",
    "and LTRIM(RTRIM(EAN)) not like '%[^0-9]%'\n",
    "and DATALENGTH(LTRIM(RTRIM(EAN))) <= '14'\n",
    "and DATALENGTH(LTRIM(RTRIM(EAN))) >= '8'\n",
    "\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "set _gtin_pruefziffer_ = dbo._GTIN_CHECK_(_gtin_)\n",
    "\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "set _gtin_pruefung_kommentar_ = \n",
    "    case \n",
    "    when _gtin_pruefziffer_ = '0' \n",
    "    then 'Von GHX / vom Lieferanten angegebene Prüfziffer ist - fehlerhaft' \n",
    "    when _gtin_pruefziffer_ = '1' then 'Von GHX / vom Lieferanten angegebene Prüfziffer ist - korrekt'\n",
    "    else 'Keine gtin vorhanden' end\n",
    "\n",
    "\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "set _hibc_ = LTRIM(RTRIM(EAN)) \n",
    "from [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "where LTRIM(RTRIM(EAN)) IS NOT NULL      \n",
    "and LTRIM(RTRIM(EAN)) LIKE '+%'\n",
    "\n",
    "\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "SET [_ean_kommentar_] = CASE\n",
    "    when LTRIM(RTRIM(EAN)) IS NOT NULL\n",
    "    and LTRIM(RTRIM(EAN)) not like '%[^0-9]%'\n",
    "    and DATALENGTH(LTRIM(RTRIM(EAN))) <= '14'\n",
    "    and DATALENGTH(LTRIM(RTRIM(EAN))) >= '8'\n",
    "    THEN 'GTIN'\n",
    "    when LTRIM(RTRIM(EAN)) IS NOT NULL\n",
    "    and LTRIM(RTRIM(EAN)) LIKE '+%'\n",
    "    THEN 'HIBC'\n",
    "    WHEN LTRIM(RTRIM(EAN)) LIKE\t'%;%'\n",
    "    THEN 'Mehrere Inhalte für diese Zeile angegeben'\n",
    "    WHEN LTRIM(RTRIM(EAN)) IS NULL\n",
    "    THEN 'Keine Angabe zu EAN'\n",
    "    ELSE 'Für diese Zeile, bisher keine Prüfung'\n",
    "    END\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ersetzt durch EAN_MASTER \n",
    "sql_ean_komm = \"\"\" UPDATE {}\n",
    "                SET [_ean_kommentar_] = CASE\n",
    "                    WHEN LTRIM(RTRIM(EAN)) NOT LIKE '%[a-zA-Z/.;:<=>@\\*_+~ ]%'\n",
    "                    THEN 'GTIN'\n",
    "                    WHEN LTRIM(RTRIM(EAN)) LIKE '+%'\n",
    "                    THEN 'HIBC'\n",
    "                    WHEN LTRIM(RTRIM(EAN)) LIKE\t'%;%'\n",
    "                    THEN 'Mehrere Inhalte für diese Zeile angegeben'\n",
    "                    WHEN LTRIM(RTRIM(EAN)) IS NULL\n",
    "                    THEN 'Keine Angabe zu EAN'\n",
    "                    ELSE 'Für diese Zeile, bisher keine Prüfung'\n",
    "                    END\n",
    "                \"\"\".format(db_tabellen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ersetzt durch EAN_MASTER\n",
    "sql_split_ean = \"\"\"\n",
    "                UPDATE {}\n",
    "                SET\n",
    "                    _gtin_ = CASE\n",
    "                    WHEN LTRIM(RTRIM(EAN)) NOT LIKE '%[a-zA-Z/.;:<=>@\\*_+~- ]%'\n",
    "                    THEN LTRIM(RTRIM(EAN))\n",
    "                    END\n",
    "                    ,_hibc_ = CASE\n",
    "                    WHEN LTRIM(RTRIM(EAN)) LIKE '+%' \n",
    "                    THEN LTRIM(RTRIM(EAN))\n",
    "                    END            \n",
    "                \"\"\".format(db_tabellen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ersetzt durch EAN_MASTER\n",
    "sql_gtin_pruefz = \"\"\"\n",
    "                WITH cte_gtin_pruefz AS (\n",
    "                SELECT _gtin_\n",
    "                       ,dbo._GTIN_CHECK_(_gtin_) _gtin_pruefziffer_\n",
    "                FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "                WHERE _gtin_ IS NOT NULL \n",
    "                       )\n",
    "                \n",
    "                UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "                SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._gtin_pruefziffer_ = cte_gtin_pruefz._gtin_pruefziffer_\n",
    "                FROM cte_gtin_pruefz\n",
    "                LEFT JOIN [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh] \n",
    "                ON [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._gtin_ =  cte_gtin_pruefz._gtin_                                                                 \n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globaler Artikel-Schlüssel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schritt 1) Globaler Artikel-Schlüssel: Übertrag Lieferant Global und Kreditoren ID Schlüssel TPSHortname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_liefID_liefGlobal_kataloge = \"\"\"\n",
    "WITH cte_lief AS (\n",
    "SELECT lief_kat.TPSHORTNAME\n",
    ",lief.LieferantenNr\n",
    ",lief.MiiLieferantGlobal\n",
    ",lief.AGKALieferanten_Lieferant\n",
    ",lief.AGKAMED_Langname\n",
    ",lief.Finale_UStID\n",
    "FROM [tbl_lieferanten] lief\n",
    "JOIN [tbl_lieferanten_kataloge] lief_kat\n",
    "ON lief.[LieferantenNr] = lief_kat.[LieferantenNr])\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh] \n",
    "SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._LieferantenNameGlobal_ =  cte_lief.MiiLieferantGlobal,\n",
    "    [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._AGKA_Kreditoren_ID_ = cte_lief.LieferantenNr,\n",
    "    [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._UStID_Vorschlag_ = cte_lief.Finale_UStID\n",
    "FROM cte_lief\n",
    "LEFT JOIN [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh] \n",
    "ON [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].TPSHORTNAME =  cte_lief.TPSHORTNAME\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schritt 2) Globaler Artikel-Schlüssel: Erstellung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_artikel_key_kataloge = \"\"\"\n",
    "WITH cte_key_field AS ( \n",
    "SELECT   _LieferantenNameGlobal_\n",
    ",[_computed_normalized_art_nr_]\n",
    ",[UOM]\n",
    ",[NOU]\n",
    ",CONCAT(_LieferantenNameGlobal_,'°',\n",
    "[_computed_normalized_art_nr_],'°',\n",
    "[UOM],'°',\n",
    "[NOU]) AS _artikel_key_\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh])\n",
    "\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_ = cte_key_field._artikel_key_\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "JOIN cte_key_field\n",
    "ON cte_key_field._LieferantenNameGlobal_ = [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._LieferantenNameGlobal_\n",
    "AND cte_key_field.[_computed_normalized_art_nr_] = [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_computed_normalized_art_nr_]\n",
    "AND cte_key_field.[UOM] = [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[UOM]\n",
    "AND cte_key_field.[NOU] = [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[NOU]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGKAMED Klassifikation JD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schritt 1) Klassifikation JD: Erstellung artikel_key für Übertrag agka_class in tbl_artikel_agkaclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_agka_class_id = \"\"\"\n",
    "WITH cte_agka_class_id_ AS (\n",
    "SELECT [Lieferantennummer_vom_Lieferanten]\n",
    ",[Artikelnummer_vom_Lieferanten]\n",
    ",tbl_art_class.[AGKA_Klassifikation_ergaenzt_korrigiert_JD]\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_artikel_agkaClass_jd] tbl_art_class)\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._agka_class_id_ = cte_agka_class_id_.[AGKA_Klassifikation_ergaenzt_korrigiert_JD]\n",
    "FROM cte_agka_class_id_\n",
    "RIGHT JOIN [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "ON  [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_AGKA_Kreditoren_ID_] = cte_agka_class_id_.[Lieferantennummer_vom_Lieferanten]\n",
    "AND [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_computed_normalized_art_nr_] = cte_agka_class_id_.[Artikelnummer_vom_Lieferanten]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schritt 2)  Klassifikation JD: Erstellung artikel_key für Übertrag agka_class in tbl_kataloge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_artikel_key_kataloge_class = \"\"\"\n",
    "WITH cte_art_key_class_roh AS ( \n",
    "SELECT \n",
    "[_computed_normalized_art_nr_]\n",
    ",[_AGKA_Kreditoren_ID_]\n",
    ",CONCAT([_AGKA_Kreditoren_ID_], '°',[_computed_normalized_art_nr_]) _artikel_key_class_\n",
    "FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh])\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_class_ = cte_art_key_class_roh._artikel_key_class_\n",
    "FROM cte_art_key_class_roh\n",
    "JOIN [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "ON [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._computed_normalized_art_nr_ = cte_art_key_class_roh._computed_normalized_art_nr_\n",
    "AND [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_AGKA_Kreditoren_ID_] = cte_art_key_class_roh.[_AGKA_Kreditoren_ID_]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schritt 3)  Klassifikation JD: Übertrag agka_class_id JOIN über Artikel_class_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_transfer_agka_class = \"\"\"\n",
    "WITH cte_transfer_agka_class AS ( \n",
    "SELECT _artikel_key_class_\n",
    ",AGKA_Klassifikation_ergaenzt_korrigiert_JD\n",
    "FROM tbl_artikel_agkaClass_jd)\n",
    "\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._agka_class_id_ = cte_transfer_agka_class.AGKA_Klassifikation_ergaenzt_korrigiert_JD\n",
    "FROM cte_transfer_agka_class\n",
    "JOIN [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "ON [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]._artikel_key_class_ = cte_transfer_agka_class._artikel_key_class_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_agkaClass_Bez_in_ghx = \"\"\"\n",
    "\n",
    "WITH cte_agkaClass_baum AS (\n",
    "\n",
    "SELECT [AgkClassID]\n",
    "      ,[AGKclassCode-LEVEL1]\n",
    "      ,[AGKclassCode-LEVEL2]\n",
    "      ,[AGKclassCode-LEVEL3]\n",
    "      ,[AGKclassCode-LEVEL4]\n",
    " FROM [Vorlauf_DB].[dbo].[tbl_agkaClass_baum]),\n",
    "\n",
    " cte_ghx_roh AS (\n",
    " SELECT [_agka_class_id_]\n",
    "  FROM [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]),\n",
    "  \n",
    "cte_roh_inkl_agka AS (\n",
    "SELECT  cte_ghx_roh.[_agka_class_id_] \n",
    ",cte_agkaClass_baum.[AGKclassCode-LEVEL1]\n",
    ",cte_agkaClass_baum.[AGKclassCode-LEVEL2]\n",
    ",cte_agkaClass_baum.[AGKclassCode-LEVEL3]\n",
    ",cte_agkaClass_baum.[AGKclassCode-LEVEL4]\n",
    "FROM cte_ghx_roh\n",
    "JOIN cte_agkaClass_baum\n",
    "ON cte_ghx_roh.[_agka_class_id_] = cte_agkaClass_baum.[AgkClassID])\n",
    "\n",
    "UPDATE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh] \n",
    "SET [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_AGKclassCode-LEVEL1_] = cte_roh_inkl_agka.[AGKclassCode-LEVEL1]\n",
    "    ,[Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_AGKclassCode-LEVEL2_] = cte_roh_inkl_agka.[AGKclassCode-LEVEL2]\n",
    "    ,[Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_AGKclassCode-LEVEL3_] = cte_roh_inkl_agka.[AGKclassCode-LEVEL3]\n",
    "    ,[Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_AGKclassCode-LEVEL4_] = cte_roh_inkl_agka.[AGKclassCode-LEVEL4]\n",
    "FROM cte_roh_inkl_agka\n",
    "LEFT JOIN [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh] \n",
    "ON [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh].[_agka_class_id_] =  cte_roh_inkl_agka.[_agka_class_id_]     \n",
    "   \n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Index erstellen (Autoincrement je Tabelle)\n",
    "sql_handle_kataloge_id = \"\"\"\n",
    "IF EXISTS (SELECT TABLE_NAME\n",
    "               FROM   Vorlauf_DB.INFORMATION_SCHEMA.COLUMNS\n",
    "               WHERE  TABLE_NAME = 'tbl_ghx_kataloge_roh'\n",
    "                      AND COLUMN_NAME = 'tbl_index'\n",
    "                      AND TABLE_SCHEMA='DBO')\n",
    "  BEGIN\n",
    "      ALTER TABLE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "      DROP COLUMN tbl_index\n",
    "      PRINT 'Spalte tbl_index gelöscht.'\n",
    "      ALTER TABLE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "      ADD tbl_index INT NOT NULL IDENTITY (1,1)\n",
    "      PRINT 'Spalte nach dem Löschen hinzugefügt'\n",
    "  END\n",
    "ELSE \n",
    "BEGIN\n",
    "    ALTER TABLE [Vorlauf_DB].[dbo].[tbl_ghx_kataloge_roh]\n",
    "    ADD tbl_index INT NOT NULL IDENTITY (1,1)\n",
    "    PRINT 'Spalte tbl_index inkl. auto_increment Werte hinzugefügt.'\n",
    "END\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen zur Steuerung SQL-Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sql_globaler_artikel():\n",
    "    \"\"\" Update GHX-Kataloge: Erstelle Globalen Artikel \"\"\"\n",
    "    start = time.process_time()\n",
    "    _sql_executer(sql_liefID_liefGlobal_kataloge)\n",
    "    _sql_executer(sql_artikel_key_kataloge)\n",
    "    print(\"\\nGlobaler Artikel-Key wurde erstellt.\")\n",
    "    print(time.process_time() - start)  \n",
    "\n",
    "\n",
    "def _sql_agka_class():\n",
    "    \"\"\" Update GHX-Kataloge / artikel-agkaClass-JD: AGKAMED-Class \"\"\"\n",
    "    start = time.process_time()\n",
    "    _sql_executer(sql_agka_class_id)\n",
    "    _sql_executer(sql_artikel_key_kataloge_class)\n",
    "    _sql_executer(sql_transfer_agka_class)\n",
    "    _sql_executer(sql_agkaClass_Bez_in_ghx)\n",
    "\n",
    "    print(\"\\nAGKAMED Class-ID in GHX-Kataloge wurde übertragen.\")\n",
    "    print(time.process_time() - start)  \n",
    "\n",
    "\n",
    "def _sql_import_kataloge():\n",
    "    \"\"\" Obligatorisch bei Katalog-Import\"\"\"\n",
    "    start = time.process_time()\n",
    "    _sql_executer(sql_delete_empty_rows)\n",
    "    _sql_executer(sql_set_import_flag)\n",
    "    _sql_executer(sql_ean_handling_master)\n",
    "    # ##########################\n",
    "    # Wird ersetzt durch master\n",
    "    #_sql_executer(sql_ean_komm)\n",
    "    #_sql_executer(sql_split_ean) \n",
    "    #_sql_executer(sql_gtin_pruefz) # GTIN-Prüfziffer Testlauf\n",
    "    # ##########################\n",
    "    _sql_executer(sql_handle_kataloge_id)\n",
    "    print(\"\\nObligatorische Anpassung nach ZIP Import wurde durchgeführt.\")\n",
    "    print(time.process_time() - start)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steuerungsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_steuerung():\n",
    "    \"\"\"\n",
    "    Steuerung der Ausführung gewünschter Funktionen\n",
    "    \n",
    "    Funktion zum Ausführen der Schritte kopieren und entpacken der zips, \\n\n",
    "    einladen in DB, löschen der Ordner in einem Schritt. \n",
    "    Erforderlich um Scope auf relevante zip-Dateien in beiden Prozessen gleich zu halten.\n",
    "    \n",
    "    INPUT:\n",
    "        --\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    start_zip_handling = _start_zip_handler()\n",
    "    \n",
    "    if start_zip_handling == False:\n",
    "        try:\n",
    "            print(\"Ordnerinhalte ungleich. Zip Handler wird gestartet.\")\n",
    "            df_relev_zips = pd.DataFrame(_erstelle_inload_liste(),dtype=str) \n",
    "            _kopiere_relev_zips_from_smb() \n",
    "            _extract_folder_from_zips(df_relev_zips,'Zip_Name_ID')\n",
    "        except Exception as e:\n",
    "            print(\"Achtung: Scheinbar fehlt im Arbeitsordner nicht die letzte .zip-Datei, sondern eine zwischendrin.\"\n",
    "                  \"\\nDas führt zum Absturz des Programms, da der Importer start jedoch nicht die korrekte zip identifiziert\")\n",
    "        \n",
    "    else: \n",
    "        print(\"Ordnerinhalte gleich. Durchlauf wird beendet, da Arbeitsordner aktuell ist\"\n",
    "              \"\\nund keine Updates vorhanden sind.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_ausfuehrung():\n",
    "    \"\"\"\n",
    "    Steuerung / Ausführung SQL-Skripte: Tabellen-Anpassungen\n",
    "    INPUT:\n",
    "        --\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _sql_import_kataloge() \n",
    "        _sql_globaler_artikel()\n",
    "#        _sql_agka_class() \n",
    "    except Exception as e:\n",
    "        print('Fehler bei der Ausführung der SQL-Skripte aufgetreten')\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "    ## METATABELLE COVIN \n",
    "#     _sql_executer(sql_cr_tbl_verw_inload_kataloge)\n",
    "#     _sql_executer(sql_bef_tbl_verw_inload_kataloge)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stammdaten_tabs_steuerung():\n",
    "    \"\"\"\n",
    "    Steuerung / Ausführung SQL-Skripte: Import xlsx.\n",
    "    Excel-Tabellen werden regelmäßig geupdated und in DB ersetzt.\n",
    "    INPUT:\n",
    "        --\n",
    "    OUTPUT:\n",
    "        --\n",
    "    \"\"\"\n",
    "    \n",
    "    ## ##############\n",
    "    ## IMPORT-PROZESS\n",
    "    ## ##############\n",
    "    \n",
    "    try:\n",
    "        #_fill_xlsx_in_ms_sql('tbl_covin_validierung') # Bei Komplett Inload\n",
    "        #_fill_xlsx_in_ms_sql('tbl_lieferanten_master') # Bei Komplett Inload\n",
    "        #_fill_xlsx_in_ms_sql('tbl_artikel_agkaClass_jd') # Bei Komplett Inload\n",
    "        #_fill_xlsx_in_ms_sql('tbl_agkaClass_jd') # Bei Komplett Inload\n",
    "        #_fill_xlsx_in_ms_sql('tbl_agkaClass_baum') # Bei Komplett Inload\n",
    "\n",
    "        # _export_warenkorb()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Fehler bei Import xlsx.\")\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Steuerung aller Funktionen\n",
    "    \"\"\"\n",
    "##     -------------------------------------   \n",
    "##     1) Verarbeitung / Inload GHX Kataloge\n",
    "##     -------------------------------------    \n",
    "#    zip_steuerung() # Für GHX INLOAD aktivieren\n",
    "\n",
    "##     ----------------------------------------------     \n",
    "##     2) Verarbeitung / Inload weitere Excel-Dateien\n",
    "##     ----------------------------------------------      \n",
    "    stammdaten_tabs_steuerung()\n",
    "\n",
    "##     --------------------------------------\n",
    "##     3) Verbindung / Update der DB-Tabellen\n",
    "##     --------------------------------------\n",
    "#    sql_ausfuehrung() # Für GHX INLOAD aktivieren\n",
    "\n",
    "##     -------------------------------------------     \n",
    "##     Müsste hierher ausgelagert werden, da stammdaten_tabs und sql_querys vorher bearbeitet werden sollten. \n",
    "##     Warenkorb-Mapping ist ein separater Prozess\n",
    "##     -------------------------------------------  \n",
    "#    _export_warenkorb() \n",
    "    \n",
    "    \n",
    "    # IMPORT SCHEDULE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LieferantenNrAGKA\n",
      "LieferantenNameGlobal\n",
      "LieferantenArtikelNr\n",
      "\n",
      "Warenkorb Mapping abgeschlossen.\n",
      "die KATALOGMAPPING_eCl@ss.xlsx Datei liegt bereit.\n",
      "292.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # main()\n",
    "    \n",
    "   # schedule.every().day.at('03:30').do(main)\n",
    "\n",
    "    #while True:\n",
    "    #    schedule.run_pending()\n",
    "    #    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_export_warenkorb():\n",
    "    _export_warenkorb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_export_warenkorb_gtin():\n",
    "    _export_warenkorb_gtin()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}